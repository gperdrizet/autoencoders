{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5467968",
   "metadata": {},
   "source": [
    "# Anomaly detection with autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6b3c8f",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Train an autoencoder to detect anomalies by learning what \"normal\" data looks like. Anything that deviates significantly from this learned pattern is flagged as anomalous.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. **Train on normal data only**: We'll use the TF Flowers dataset (all 5 flower types) as our \"normal\" training data\n",
    "2. **Test on anomalies**: We'll use COCO dataset images (cars, people, animals, food, etc.) as anomalies\n",
    "3. **Detection method**: High reconstruction error indicates the image is NOT flower-like, therefore anomalous\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "The autoencoder learns to compress and reconstruct flower images efficiently. When shown non-flower images, it can't reconstruct them well because they don't match the learned flower patterns. This reconstruction error becomes our anomaly score!\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "- Quality control in manufacturing\n",
    "- Fraud detection in financial transactions\n",
    "- Network intrusion detection\n",
    "- Medical imaging (detecting abnormal scans)\n",
    "\n",
    "## 1. Notebook setup\n",
    "\n",
    "### 1.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793079b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Local imports\n",
    "from src.data_utils import FLOWER_CLASSES, COCO_CLASSES, load_flowers, load_coco_cached\n",
    "from src.metrics import (\n",
    "    calculate_reconstruction_error,\n",
    "    calculate_psnr,\n",
    "    calculate_ssim,\n",
    "    compute_roc_metrics,\n",
    "    find_optimal_threshold\n",
    ")\n",
    "from src.model_utils import build_compression_ae\n",
    "from src.visualization import plot_image_grid, plot_roc_curve, plot_error_histogram\n",
    "from src.huggingface_utils import download_model, upload_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046c29b",
   "metadata": {},
   "source": [
    "### 1.2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAIN_MODEL = True  # Set to False to download pre-trained model from HuggingFace\n",
    "\n",
    "# Training hyperparameters\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# GPU configuration\n",
    "GPU_ID = 0  # Which GPU to use (0-indexed). Set to None to use all available GPUs.\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create logs directory for TensorBoard\n",
    "logs_dir = Path('../logs')\n",
    "logs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        if GPU_ID is not None:\n",
    "            # Use specific GPU\n",
    "            tf.config.set_visible_devices(gpus[GPU_ID], 'GPU')\n",
    "            tf.config.experimental.set_memory_growth(gpus[GPU_ID], True)\n",
    "            print(f'Using GPU {GPU_ID}: {gpus[GPU_ID].name}')\n",
    "        else:\n",
    "            # Use all GPUs with memory growth\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f'Using {len(gpus)} GPU(s): {[gpu.name for gpu in gpus]}')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('No GPU available, using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482cbb7",
   "metadata": {},
   "source": [
    "## 2. Data preparation\n",
    "\n",
    "We'll train on the **TF Flowers dataset** (all 5 classes) as 'normal' data, then test anomaly detection using **non-flower images** from COCO.\n",
    "\n",
    "This demonstrates the key insight: train on what's normal, detect anything different as anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51759e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load flowers dataset (all classes as \"normal\")\n",
    "(x_train_flowers, y_train_flowers), (x_test_flowers, y_test_flowers) = load_flowers(normalize=True)\n",
    "\n",
    "print('Flowers Dataset (Normal Images):')\n",
    "print(f'  Training samples: {x_train_flowers.shape}')\n",
    "print(f'  Test samples: {x_test_flowers.shape}')\n",
    "print(f'  Classes: {FLOWER_CLASSES}')\n",
    "print(f'  All flower images will be considered NORMAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec41b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO images as anomalies (non-flower images)\n",
    "# Using cached version for faster loading\n",
    "(_, _), (x_coco, y_coco) = load_coco_cached(subset_percent=10, normalize=True)\n",
    "\n",
    "# Sample 500 random COCO images for anomaly testing\n",
    "n_anomaly_samples = 500\n",
    "anomaly_indices = np.random.choice(len(x_coco), n_anomaly_samples, replace=False)\n",
    "x_test_anomaly = x_coco[anomaly_indices]\n",
    "y_test_anomaly = y_coco[anomaly_indices]\n",
    "\n",
    "print('\\nCOCO Dataset (Anomalous Images):')\n",
    "print(f'  Anomaly test samples: {len(x_test_anomaly)}')\n",
    "print(f'  Number of COCO classes: {len(COCO_CLASSES)}')\n",
    "print(f'  Sample categories in test set: {[COCO_CLASSES[y] for y in y_test_anomaly[:10]]}')\n",
    "\n",
    "# Organize data for training and evaluation\n",
    "x_train_normal = x_train_flowers\n",
    "x_test_normal = x_test_flowers\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('Dataset Summary:')\n",
    "print('='*60)\n",
    "print(f'Training (normal flowers): {len(x_train_normal)} samples')\n",
    "print(f'Test normal (flowers): {len(x_test_normal)} samples')\n",
    "print(f'Test anomaly (COCO non-flowers): {len(x_test_anomaly)} samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e10e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize normal (flowers) vs anomalous (non-flowers) samples\n",
    "fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "\n",
    "# Normal samples (flowers)\n",
    "normal_indices = np.random.choice(len(x_test_normal), 10, replace=False)\n",
    "for i, idx in enumerate(normal_indices):\n",
    "    axes[0, i].imshow(x_test_normal[idx])\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('Normal\\n(Flowers)', fontweight='bold', fontsize=12, rotation=0, labelpad=50, va='center')\n",
    "\n",
    "# Anomalous samples (COCO)\n",
    "anomaly_indices_viz = np.random.choice(len(x_test_anomaly), 10, replace=False)\n",
    "for i, idx in enumerate(anomaly_indices_viz):\n",
    "    axes[1, i].imshow(x_test_anomaly[idx])\n",
    "    axes[1, i].axis('off')\n",
    "    # Show category name\n",
    "    category = COCO_CLASSES[y_test_anomaly[idx]]\n",
    "    axes[1, i].set_title(category, fontsize=9, color='red')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('Anomalous\\n(COCO)', fontweight='bold', fontsize=12, rotation=0, labelpad=50, va='center', color='red')\n",
    "\n",
    "plt.suptitle('Normal (Flowers) vs Anomalous (Non-Flower Images)', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fbd9dc",
   "metadata": {},
   "source": [
    "## Build and Train Anomaly Detection Model\n",
    "\n",
    "**Key Concept**: We train ONLY on flower images (normal data). The model learns to reconstruct flowers very well.\n",
    "\n",
    "When we later show it non-flower images (cars, people, food, etc.), it struggles to reconstruct them accurately, producing high reconstruction error. This is how we detect anomalies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589003ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "logs_dir = Path('../logs')\n",
    "logs_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-or-download",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    print('Downloading pre-trained model from HuggingFace...')\n",
    "    model_path = download_model('anomaly_ae.keras', models_dir='../models')\n",
    "    \n",
    "    from tensorflow import keras\n",
    "    autoencoder = keras.models.load_model(model_path)\n",
    "    print('\u2713 Model loaded! Skipping training.')\n",
    "    print('Set TRAIN_MODEL=True to train your own model.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "autoencoder = build_anomaly_ae(latent_dim=LATENT_DIM)\n",
    "\n",
    "# Compile\n",
    "autoencoder.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(f\"Total parameters: {autoencoder.count_params():,}\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=str(models_dir / 'anomaly_ae.keras'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=str(logs_dir / 'anomaly_detection'),\n",
    "        histogram_freq=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train on NORMAL data only\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Anomaly Detection Autoencoder\")\n",
    "print(\"Training on NORMAL classes only!\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    x_train_normal, x_train_normal,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(x_test_normal, x_test_normal),  # Validate on normal data\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nModel saved to: models/anomaly_ae.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8298e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig = plot_training_history(history)\n",
    "plt.suptitle('Anomaly Detection Model Training History', fontsize=14, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9777a84e",
   "metadata": {},
   "source": [
    "## Compute Reconstruction Errors\n",
    "\n",
    "Now we'll compute reconstruction errors for both normal and anomalous test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd820098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate reconstruction errors\n",
    "print(\"Computing reconstruction errors...\")\n",
    "\n",
    "errors_normal = calculate_reconstruction_error(autoencoder, x_test_normal)\n",
    "errors_anomaly = calculate_reconstruction_error(autoencoder, x_test_anomaly)\n",
    "\n",
    "print(f\"\\nNormal samples:\")\n",
    "print(f\"  Mean error: {np.mean(errors_normal):.6f}\")\n",
    "print(f\"  Std error: {np.std(errors_normal):.6f}\")\n",
    "print(f\"  Min error: {np.min(errors_normal):.6f}\")\n",
    "print(f\"  Max error: {np.max(errors_normal):.6f}\")\n",
    "\n",
    "print(f\"\\nAnomalous samples:\")\n",
    "print(f\"  Mean error: {np.mean(errors_anomaly):.6f}\")\n",
    "print(f\"  Std error: {np.std(errors_anomaly):.6f}\")\n",
    "print(f\"  Min error: {np.min(errors_anomaly):.6f}\")\n",
    "print(f\"  Max error: {np.max(errors_anomaly):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943cb28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(errors_normal, bins=50, alpha=0.7, label='Normal', color='blue', density=True)\n",
    "axes[0].hist(errors_anomaly, bins=50, alpha=0.7, label='Anomaly', color='red', density=True)\n",
    "axes[0].set_xlabel('Reconstruction Error (MSE)', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].set_title('Distribution of Reconstruction Errors', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot([errors_normal, errors_anomaly], labels=['Normal', 'Anomaly'])\n",
    "axes[1].set_ylabel('Reconstruction Error (MSE)', fontsize=12)\n",
    "axes[1].set_title('Error Distribution Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a32d796",
   "metadata": {},
   "source": [
    "## ROC Analysis and Threshold Selection\n",
    "\n",
    "We'll use ROC (Receiver Operating Characteristic) analysis to:\n",
    "1. Evaluate detection performance\n",
    "2. Find the optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27415bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels and scores\n",
    "y_true = np.concatenate([\n",
    "    np.zeros(len(errors_normal)),  # Normal = 0\n",
    "    np.ones(len(errors_anomaly))   # Anomaly = 1\n",
    "])\n",
    "\n",
    "scores = np.concatenate([errors_normal, errors_anomaly])\n",
    "\n",
    "# Compute ROC metrics\n",
    "roc_metrics = compute_roc_metrics(y_true, scores)\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_metrics['auc']:.4f}\")\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_threshold = find_optimal_threshold(y_true, scores)\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.6f}\")\n",
    "\n",
    "# Also compute percentile-based threshold\n",
    "percentile_threshold = compute_anomaly_threshold(errors_normal, percentile=95)\n",
    "print(f\"95th Percentile Threshold: {percentile_threshold:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b711fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ax.plot(roc_metrics['fpr'], roc_metrics['tpr'], linewidth=2, \n",
    "        label=f\"ROC Curve (AUC = {roc_metrics['auc']:.4f})\")\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curve - Anomaly Detection', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ec4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with optimal threshold\n",
    "y_pred = (scores > optimal_threshold).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = compute_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Calculate metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANOMALY DETECTION PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives:  {tn:5d}\")\n",
    "print(f\"  False Positives: {fp:5d}\")\n",
    "print(f\"  False Negatives: {fn:5d}\")\n",
    "print(f\"  True Positives:  {tp:5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dedbc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Anomaly'],\n",
    "            yticklabels=['Normal', 'Anomaly'],\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            ax=ax, annot_kws={'size': 16})\n",
    "\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb3e090",
   "metadata": {},
   "source": [
    "## Visual Analysis: Top Anomalies and Errors\n",
    "\n",
    "Let's examine which samples have the highest and lowest reconstruction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top anomalies (highest errors in anomaly set)\n",
    "top_anomaly_indices = np.argsort(errors_anomaly)[-10:][::-1]\n",
    "top_anomaly_errors = errors_anomaly[top_anomaly_indices]\n",
    "top_anomaly_images = x_test_anomaly[top_anomaly_indices]\n",
    "\n",
    "# Find false negatives (low errors in anomaly set)\n",
    "false_neg_indices = np.argsort(errors_anomaly)[:10]\n",
    "false_neg_errors = errors_anomaly[false_neg_indices]\n",
    "false_neg_images = x_test_anomaly[false_neg_indices]\n",
    "\n",
    "# Find false positives (high errors in normal set)\n",
    "false_pos_indices = np.argsort(errors_normal)[-10:][::-1]\n",
    "false_pos_errors = errors_normal[false_pos_indices]\n",
    "false_pos_images = x_test_normal[false_pos_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(3, 10, figsize=(20, 6))\n",
    "\n",
    "# Top anomalies (correctly detected)\n",
    "for i in range(10):\n",
    "    axes[0, i].imshow(top_anomaly_images[i])\n",
    "    axes[0, i].set_title(f'{top_anomaly_errors[i]:.4f}', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "axes[0, 0].set_ylabel('Top Anomalies\\n(Correctly Detected)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# False negatives (anomalies that look normal)\n",
    "for i in range(10):\n",
    "    axes[1, i].imshow(false_neg_images[i])\n",
    "    axes[1, i].set_title(f'{false_neg_errors[i]:.4f}', fontsize=10, color='orange')\n",
    "    axes[1, i].axis('off')\n",
    "axes[1, 0].set_ylabel('False Negatives\\n(Missed Anomalies)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# False positives (normal that look anomalous)\n",
    "for i in range(10):\n",
    "    axes[2, i].imshow(false_pos_images[i])\n",
    "    axes[2, i].set_title(f'{false_pos_errors[i]:.4f}', fontsize=10, color='red')\n",
    "    axes[2, i].axis('off')\n",
    "axes[2, 0].set_ylabel('False Positives\\n(Normal Flagged)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Error Analysis: Top-10 Examples by Category', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab27d3",
   "metadata": {},
   "source": [
    "## Reconstruction Visualization\n",
    "\n",
    "Compare how well the model reconstructs normal vs anomalous images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select samples\n",
    "n_samples = 5\n",
    "\n",
    "# Normal samples\n",
    "normal_samples = x_test_normal[np.random.choice(len(x_test_normal), n_samples, replace=False)]\n",
    "normal_reconstructions = autoencoder.predict(normal_samples, verbose=0)\n",
    "\n",
    "# Anomaly samples\n",
    "anomaly_samples = x_test_anomaly[np.random.choice(len(x_test_anomaly), n_samples, replace=False)]\n",
    "anomaly_reconstructions = autoencoder.predict(anomaly_samples, verbose=0)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(4, n_samples, figsize=(n_samples * 3, 12))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Normal originals\n",
    "    axes[0, i].imshow(normal_samples[i])\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Normal\\nOriginal', fontweight='bold')\n",
    "    \n",
    "    # Normal reconstructions\n",
    "    axes[1, i].imshow(normal_reconstructions[i])\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('Normal\\nReconstructed', fontweight='bold')\n",
    "    \n",
    "    # Anomaly originals\n",
    "    axes[2, i].imshow(anomaly_samples[i])\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[2, i].set_title('Anomaly\\nOriginal', fontweight='bold', color='red')\n",
    "    \n",
    "    # Anomaly reconstructions\n",
    "    axes[3, i].imshow(anomaly_reconstructions[i])\n",
    "    axes[3, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[3, i].set_title('Anomaly\\nReconstructed', fontweight='bold', color='red')\n",
    "\n",
    "plt.suptitle('Reconstruction Quality: Normal vs Anomalous', fontsize=16, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f767b7",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Effective Separation**: The autoencoder successfully distinguishes between flower images (normal) and non-flower images (anomalous) based on reconstruction error alone.\n",
    "\n",
    "2. **High Performance**: Achieved strong ROC-AUC, demonstrating the model learned flower-specific features that don't transfer to arbitrary objects.\n",
    "\n",
    "3. **Interpretable**: Reconstruction errors provide interpretable anomaly scores - higher error means \"less flower-like\".\n",
    "\n",
    "4. **Generalization**: This approach works because the model learned what flowers look like. Anything that doesn't match that learned pattern (cars, people, food, etc.) gets high reconstruction error.\n",
    "\n",
    "### Real-World Applications:\n",
    "\n",
    "- **Manufacturing Quality Control**: Detect defective products that look different from normal items\n",
    "- **Security & Fraud Detection**: Identify unusual patterns in network traffic or transactions  \n",
    "- **Medical Imaging**: Flag abnormal scans that deviate from healthy patterns\n",
    "- **Content Moderation**: Detect inappropriate content by training on appropriate content only\n",
    "\n",
    "### Interactive Demo:\n",
    "\n",
    "Try the Streamlit app where you can:\n",
    "- Upload your own images to test if they're \"flower-like\"\n",
    "- See reconstruction errors in real-time\n",
    "- Adjust detection thresholds\n",
    "- Test edge cases (potted plants, gardens, flower paintings, etc.)\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- Requires clean normal data for training (no anomalies in training set)\n",
    "- Threshold selection affects false positive/negative trade-off\n",
    "- May struggle with subtle anomalies or edge cases (e.g., flower paintings)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Try uploading various images in the Streamlit app\n",
    "2. Experiment with different latent dimensions\n",
    "3. Explore Variational Autoencoders (VAEs) for probabilistic anomaly scores"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}