{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e2f238",
   "metadata": {},
   "source": [
    "# Denoising autoencoder\n",
    "\n",
    "## 1. Notebook setup\n",
    "\n",
    "### 1.1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3622540",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Train a denoising autoencoder to remove noise from images and restore clean versions.\n",
    "\n",
    "## What is a Denoising Autoencoder?\n",
    "\n",
    "A denoising autoencoder learns to:\n",
    "1. Take a **noisy image** as input\n",
    "2. Reconstruct the **clean, original image** as output\n",
    "\n",
    "This forces the network to learn robust features that can distinguish signal from noise.\n",
    "\n",
    "## Applications\n",
    "\n",
    "- Image restoration and enhancement\n",
    "- Removing artifacts from photos\n",
    "- Preprocessing for computer vision tasks\n",
    "- Feature learning for downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0db006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Local imports\n",
    "from src.data_utils import load_coco\n",
    "from src.visualization import plot_image_grid\n",
    "from src.metrics import calculate_psnr, calculate_ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95daa795",
   "metadata": {},
   "source": [
    "### 1.2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f59d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Noise parameters\n",
    "NOISE_FACTOR = 0.25\n",
    "\n",
    "# GPU configuration\n",
    "GPU_ID = 0  # Which GPU to use (0-indexed). Set to None to use all available GPUs.\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        if GPU_ID is not None:\n",
    "            # Use specific GPU\n",
    "            tf.config.set_visible_devices(gpus[GPU_ID], 'GPU')\n",
    "            tf.config.experimental.set_memory_growth(gpus[GPU_ID], True)\n",
    "            print(f'Using GPU {GPU_ID}: {gpus[GPU_ID].name}')\n",
    "        else:\n",
    "            # Use all GPUs with memory growth\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f'Using {len(gpus)} GPU(s): {[gpu.name for gpu in gpus]}')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('No GPU available, using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7561a",
   "metadata": {},
   "source": [
    "## 2. Data preparation\n",
    "\n",
    "We'll use a 10% subset of the COCO 2017 dataset (~11,800 diverse images across 80 categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0caa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO dataset (10% subset)\n",
    "(x_train, y_train), (x_test, y_test) = load_coco(subset_percent=10)\n",
    "\n",
    "print(f'Training set: {x_train.shape}')\n",
    "print(f'Test set: {x_test.shape}')\n",
    "print(f'Value range: [{x_train.min():.2f}, {x_train.max():.2f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577fa3f",
   "metadata": {},
   "source": [
    "## Noise Functions\n",
    "\n",
    "We'll implement different types of noise to test the robustness of our denoising autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(images, noise_factor=0.3):\n",
    "    '''\n",
    "    Add Gaussian (normal) noise to images.\n",
    "    \n",
    "    Args:\n",
    "        images: Clean images\n",
    "        noise_factor: Standard deviation of noise (higher = more noise)\n",
    "    \n",
    "    Returns:\n",
    "        Noisy images clipped to [0, 1]\n",
    "    '''\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_factor, size=images.shape)\n",
    "    noisy_images = images + noise\n",
    "    return np.clip(noisy_images, 0.0, 1.0)\n",
    "\n",
    "def add_salt_pepper_noise(images, amount=0.05):\n",
    "    '''\n",
    "    Add salt and pepper noise (random black and white pixels).\n",
    "    \n",
    "    Args:\n",
    "        images: Clean images\n",
    "        amount: Fraction of pixels to corrupt\n",
    "    \n",
    "    Returns:\n",
    "        Noisy images\n",
    "    '''\n",
    "    noisy_images = images.copy()\n",
    "    \n",
    "    # Salt (white pixels)\n",
    "    num_salt = int(amount * images.size * 0.5)\n",
    "    coords = [np.random.randint(0, i - 1, num_salt) for i in images.shape]\n",
    "    noisy_images[coords[0], coords[1], coords[2], coords[3]] = 1.0\n",
    "    \n",
    "    # Pepper (black pixels)\n",
    "    num_pepper = int(amount * images.size * 0.5)\n",
    "    coords = [np.random.randint(0, i - 1, num_pepper) for i in images.shape]\n",
    "    noisy_images[coords[0], coords[1], coords[2], coords[3]] = 0.0\n",
    "    \n",
    "    return noisy_images\n",
    "\n",
    "def add_speckle_noise(images, noise_factor=0.2):\n",
    "    '''\n",
    "    Add speckle (multiplicative) noise.\n",
    "    \n",
    "    Args:\n",
    "        images: Clean images\n",
    "        noise_factor: Scale of noise\n",
    "    \n",
    "    Returns:\n",
    "        Noisy images clipped to [0, 1]\n",
    "    '''\n",
    "    noise = np.random.randn(*images.shape) * noise_factor\n",
    "    noisy_images = images + images * noise\n",
    "    return np.clip(noisy_images, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b1977",
   "metadata": {},
   "source": [
    "## Visualize Different Noise Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample image\n",
    "sample_idx = 42\n",
    "sample_image = x_train[sample_idx:sample_idx+1]\n",
    "\n",
    "# Create noisy versions\n",
    "gaussian_noisy = add_gaussian_noise(sample_image, noise_factor=0.3)\n",
    "salt_pepper_noisy = add_salt_pepper_noise(sample_image, amount=0.05)\n",
    "speckle_noisy = add_speckle_noise(sample_image, noise_factor=0.2)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "axes[0].imshow(sample_image[0])\n",
    "axes[0].set_title('Original', fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(gaussian_noisy[0])\n",
    "axes[1].set_title('Gaussian Noise', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(salt_pepper_noisy[0])\n",
    "axes[2].set_title('Salt & Pepper Noise', fontsize=14)\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(speckle_noisy[0])\n",
    "axes[3].set_title('Speckle Noise', fontsize=14)\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e85838",
   "metadata": {},
   "source": [
    "## Create Training Data with Noise\n",
    "\n",
    "For this example, we'll focus on Gaussian noise as it's the most common type in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f3de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create noisy training and test sets\n",
    "noise_factor = 0.25\n",
    "\n",
    "x_train_noisy = add_gaussian_noise(x_train, noise_factor=noise_factor)\n",
    "x_test_noisy = add_gaussian_noise(x_test, noise_factor=noise_factor)\n",
    "\n",
    "print(f'Created noisy datasets with noise factor: {noise_factor}')\n",
    "print(f'Noisy training set: {x_train_noisy.shape}')\n",
    "print(f'Noisy test set: {x_test_noisy.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ed709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clean vs noisy samples\n",
    "n_samples = 8\n",
    "indices = np.random.choice(len(x_train), n_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, n_samples, figsize=(20, 5))\n",
    "fig.suptitle('Clean Images (top) vs Noisy Images (bottom)', fontsize=16, y=1.02)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[0, i].imshow(x_train[idx])\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(x_train_noisy[idx])\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ed09c",
   "metadata": {},
   "source": [
    "## Build Denoising Autoencoder\n",
    "\n",
    "The architecture is similar to a compression autoencoder, but:\n",
    "- **Input**: Noisy images\n",
    "- **Output**: Clean images\n",
    "- The bottleneck forces the network to learn noise-resistant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_denoising_autoencoder(input_shape=(64, 64, 3), latent_dim=128):\n",
    "    '''\n",
    "    Build a denoising autoencoder.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input images\n",
    "        latent_dim: Dimension of latent bottleneck\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    '''\n",
    "    # Encoder\n",
    "    encoder_input = keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(encoder_input)\n",
    "    x = layers.MaxPooling2D(2, padding='same')(x)  # 32x32\n",
    "    \n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2, padding='same')(x)  # 16x16\n",
    "    \n",
    "    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2, padding='same')(x)  # 8x8\n",
    "    \n",
    "    x = layers.Conv2D(512, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2, padding='same')(x)  # 4x4\n",
    "    \n",
    "    # Bottleneck\n",
    "    x = layers.Flatten()(x)\n",
    "    latent = layers.Dense(latent_dim, activation='relu', name='latent')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.Dense(4 * 4 * 512, activation='relu')(latent)\n",
    "    x = layers.Reshape((4, 4, 512))(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(512, 3, activation='relu', strides=2, padding='same')(x)  # 8x8\n",
    "    x = layers.Conv2DTranspose(256, 3, activation='relu', strides=2, padding='same')(x)  # 16x16\n",
    "    x = layers.Conv2DTranspose(128, 3, activation='relu', strides=2, padding='same')(x)  # 32x32\n",
    "    x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)   # 64x64\n",
    "    \n",
    "    decoder_output = layers.Conv2D(3, 3, activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    # Build model\n",
    "    autoencoder = keras.Model(encoder_input, decoder_output, name='denoising_autoencoder')\n",
    "    \n",
    "    # Compile\n",
    "    autoencoder.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ffb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = build_denoising_autoencoder(latent_dim=128)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e765f8ac",
   "metadata": {},
   "source": [
    "## Train the Denoising Autoencoder\n",
    "\n",
    "Key insight: We train on **noisy inputs** but optimize to match **clean outputs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b2ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Input: noisy images, Target: clean images\n",
    "history = model.fit(\n",
    "    x_train_noisy,  # Noisy input\n",
    "    x_train,        # Clean target\n",
    "    validation_data=(x_test_noisy, x_test),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55159a",
   "metadata": {},
   "source": [
    "## Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb963aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Train MAE', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('MAE', fontsize=12)\n",
    "axes[1].set_title('Mean Absolute Error', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfaafe4",
   "metadata": {},
   "source": [
    "## Evaluate Denoising Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ba704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoise test images\n",
    "x_test_denoised = model.predict(x_test_noisy, verbose=0)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_noisy = np.mean((x_test - x_test_noisy) ** 2)\n",
    "mse_denoised = np.mean((x_test - x_test_denoised) ** 2)\n",
    "\n",
    "psnr_noisy = calculate_psnr(x_test, x_test_noisy)\n",
    "psnr_denoised = calculate_psnr(x_test, x_test_denoised)\n",
    "\n",
    "ssim_noisy = calculate_ssim(x_test, x_test_noisy)\n",
    "ssim_denoised = calculate_ssim(x_test, x_test_denoised)\n",
    "\n",
    "print('Performance Metrics:')\n",
    "print('=' * 50)\n",
    "print(f'\\nNoisy Images vs Clean:')\n",
    "print(f'  MSE:  {mse_noisy:.6f}')\n",
    "print(f'  PSNR: {psnr_noisy:.2f} dB')\n",
    "print(f'  SSIM: {ssim_noisy:.4f}')\n",
    "\n",
    "print(f'\\nDenoised Images vs Clean:')\n",
    "print(f'  MSE:  {mse_denoised:.6f}')\n",
    "print(f'  PSNR: {psnr_denoised:.2f} dB')\n",
    "print(f'  SSIM: {ssim_denoised:.4f}')\n",
    "\n",
    "print(f'\\nImprovement:')\n",
    "print(f'  MSE:  {(mse_noisy - mse_denoised) / mse_noisy * 100:.1f}% reduction')\n",
    "print(f'  PSNR: {psnr_denoised - psnr_noisy:.2f} dB increase')\n",
    "print(f'  SSIM: {(ssim_denoised - ssim_noisy) / (1 - ssim_noisy) * 100:.1f}% closer to perfect')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02984863",
   "metadata": {},
   "source": [
    "## Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e81d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize denoising results\n",
    "n_samples = 8\n",
    "indices = np.random.choice(len(x_test), n_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, n_samples, figsize=(20, 7.5))\n",
    "fig.suptitle('Denoising Results: Original (top) vs Noisy (middle) vs Denoised (bottom)', \n",
    "             fontsize=16, y=0.995)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    # Original clean image\n",
    "    axes[0, i].imshow(x_test[idx])\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('Original', fontsize=12, rotation=0, labelpad=40, va='center')\n",
    "    \n",
    "    # Noisy image\n",
    "    axes[1, i].imshow(x_test_noisy[idx])\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('Noisy', fontsize=12, rotation=0, labelpad=40, va='center')\n",
    "    \n",
    "    # Denoised image\n",
    "    axes[2, i].imshow(x_test_denoised[idx])\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[2, i].set_ylabel('Denoised', fontsize=12, rotation=0, labelpad=40, va='center')\n",
    "    \n",
    "    # Add metrics for each image\n",
    "    psnr_val = calculate_psnr(x_test[idx:idx+1], x_test_denoised[idx:idx+1])\n",
    "    ssim_val = calculate_ssim(x_test[idx:idx+1], x_test_denoised[idx:idx+1])\n",
    "    axes[2, i].set_title(f'PSNR: {psnr_val:.1f}dB\\nSSIM: {ssim_val:.3f}', \n",
    "                         fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c04cc3",
   "metadata": {},
   "source": [
    "## Experiment: Different Noise Levels\n",
    "\n",
    "Let's test how the model performs on different noise levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c848c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on different noise levels\n",
    "noise_levels = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "results = []\n",
    "\n",
    "for noise_factor in noise_levels:\n",
    "    # Create noisy images\n",
    "    x_test_noisy_temp = add_gaussian_noise(x_test, noise_factor=noise_factor)\n",
    "    \n",
    "    # Denoise\n",
    "    x_test_denoised_temp = model.predict(x_test_noisy_temp, verbose=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    psnr_before = calculate_psnr(x_test, x_test_noisy_temp)\n",
    "    psnr_after = calculate_psnr(x_test, x_test_denoised_temp)\n",
    "    ssim_before = calculate_ssim(x_test, x_test_noisy_temp)\n",
    "    ssim_after = calculate_ssim(x_test, x_test_denoised_temp)\n",
    "    \n",
    "    results.append({\n",
    "        'noise_factor': noise_factor,\n",
    "        'psnr_before': psnr_before,\n",
    "        'psnr_after': psnr_after,\n",
    "        'ssim_before': ssim_before,\n",
    "        'ssim_after': ssim_after\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "print('\\nPerformance across different noise levels:')\n",
    "print('=' * 80)\n",
    "print(f'{\"Noise\":>8} | {\"PSNR Before\":>12} | {\"PSNR After\":>12} | {\"SSIM Before\":>12} | {\"SSIM After\":>12}')\n",
    "print('-' * 80)\n",
    "for r in results:\n",
    "    print(f'{r[\"noise_factor\"]:>8.2f} | {r[\"psnr_before\"]:>12.2f} | {r[\"psnr_after\"]:>12.2f} | '\n",
    "          f'{r[\"ssim_before\"]:>12.4f} | {r[\"ssim_after\"]:>12.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e351d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance across noise levels\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "noise_factors = [r['noise_factor'] for r in results]\n",
    "\n",
    "# PSNR\n",
    "axes[0].plot(noise_factors, [r['psnr_before'] for r in results], \n",
    "             'o-', label='Before Denoising', linewidth=2, markersize=8)\n",
    "axes[0].plot(noise_factors, [r['psnr_after'] for r in results], \n",
    "             's-', label='After Denoising', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Noise Factor', fontsize=12)\n",
    "axes[0].set_ylabel('PSNR (dB)', fontsize=12)\n",
    "axes[0].set_title('PSNR vs Noise Level', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# SSIM\n",
    "axes[1].plot(noise_factors, [r['ssim_before'] for r in results], \n",
    "             'o-', label='Before Denoising', linewidth=2, markersize=8)\n",
    "axes[1].plot(noise_factors, [r['ssim_after'] for r in results], \n",
    "             's-', label='After Denoising', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Noise Factor', fontsize=12)\n",
    "axes[1].set_ylabel('SSIM', fontsize=12)\n",
    "axes[1].set_title('SSIM vs Noise Level', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51af0be",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the denoising model\n",
    "model_path = '../models/denoising_ae_latent128.keras'\n",
    "model.save(model_path)\n",
    "print(f'Model saved to: {model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124b6cc",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Training Strategy**: Denoising autoencoders are trained with noisy inputs and clean targets\n",
    "\n",
    "2. **Robust Features**: The bottleneck forces the network to learn features that are resistant to noise\n",
    "\n",
    "3. **Generalization**: The model trained on one noise level can still denoise images with different noise levels, though performance degrades with very high noise\n",
    "\n",
    "4. **Applications**: Beyond image denoising, this approach is useful for:\n",
    "   - Pretraining encoders for other tasks\n",
    "   - Data augmentation\n",
    "   - Regularization technique\n",
    "\n",
    "5. **Limitations**: \n",
    "   - Works best when test noise is similar to training noise\n",
    "   - May blur fine details while removing noise\n",
    "   - Performance depends on bottleneck size and architecture"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
