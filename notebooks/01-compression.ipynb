{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8fe3a80",
   "metadata": {},
   "source": [
    "# Autoencoder for image compression\n",
    "\n",
    "## 1. Notebook setup\n",
    "\n",
    "### 1.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d71b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "# import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Local imports\n",
    "from src.data_utils import COCO_CLASSES, load_coco\n",
    "from src.metrics import compute_metrics_summary\n",
    "from src.model_utils import build_compression_ae\n",
    "from src.visualization import plot_image_grid, plot_reconstruction_comparison, plot_training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09814ccc",
   "metadata": {},
   "source": [
    "### 1.2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96617760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPU(s): ['/physical_device:GPU:0', '/physical_device:GPU:1']\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "latent_dims = [32, 64, 128, 256]\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "# GPU configuration\n",
    "GPU_ID = 0  # Which GPU to use (0-indexed). Set to None to use all available GPUs.\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create logs directory for TensorBoard\n",
    "logs_dir = Path('../logs')\n",
    "logs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(315)\n",
    "tf.random.set_seed(315)\n",
    "\n",
    "# Configure GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        if GPU_ID is not None:\n",
    "            # Use specific GPU\n",
    "            tf.config.set_visible_devices(gpus[GPU_ID], 'GPU')\n",
    "            tf.config.experimental.set_memory_growth(gpus[GPU_ID], True)\n",
    "            print(f'Using GPU {GPU_ID}: {gpus[GPU_ID].name}')\n",
    "        else:\n",
    "            # Use all GPUs with memory growth\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f'Using {len(gpus)} GPU(s): {[gpu.name for gpu in gpus]}')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('No GPU available, using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494cc86",
   "metadata": {},
   "source": [
    "## 2. Data preparation\n",
    "\n",
    "COCO (Common Objects in Context) is a large-scale dataset with ~118K training images across 80 diverse categories. We use a 10% subset (~11,800 images) for efficient training while maintaining diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f78860f",
   "metadata": {},
   "source": [
    "### 2.1. Load COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9eedd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder /home/vscode/tensorflow_datasets/coco/2017/1.1.0 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/vscode/tensorflow_datasets/coco/2017/1.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c9610e9c3a4870a49cc46e9a80eaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a53ef564144864bb51613d75f0cd6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93165c9e0d514b6b804adbe39c9d403a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae9c6e00b1c414998a557b6de9c9bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee96acbcf9cc49f492cee1b253315fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load COCO dataset (10% subset)\n",
    "(x_train, y_train), (x_test, y_test) = load_coco(subset_percent=10, normalize=True)\n",
    "\n",
    "print(f'Training set: {x_train.shape}')\n",
    "print(f'Test set: {x_test.shape}')\n",
    "print(f'Image shape: {x_train.shape[1:]}')\n",
    "print(f'Number of classes: {len(COCO_CLASSES)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f74aa73",
   "metadata": {},
   "source": [
    "### 2.2. Sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some random samples\n",
    "n_samples = 10\n",
    "indices = np.random.choice(len(x_test), n_samples, replace=False)\n",
    "samples = x_test[indices]\n",
    "labels = [COCO_CLASSES[y_test[i]] for i in indices]\n",
    "\n",
    "fig = plot_image_grid(samples, titles=labels)\n",
    "plt.suptitle('Random Images from COCO Test Set', fontsize=14, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c374fb9",
   "metadata": {},
   "source": [
    "## 3. Compression autoencoders\n",
    "\n",
    "We'll train separate models with different latent dimensions to explore the compression-quality trade-off:\n",
    "\n",
    "- **Latent dim 32**: Highest compression (~99.7% size reduction, 384x compression)\n",
    "- **Latent dim 64**: Very high compression (~99.5% size reduction, 192x compression)\n",
    "- **Latent dim 128**: High compression (~99% size reduction, 96x compression)\n",
    "- **Latent dim 256**: Moderate compression (~98% size reduction, 48x compression)\n",
    "\n",
    "Original image size: 64 x 64 x 3 = 12,288 pixels\n",
    "\n",
    "### 3.1. Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cdd1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for each latent dimension\n",
    "trained_models = {}\n",
    "histories = {}\n",
    "\n",
    "for latent_dim in latent_dims:\n",
    "\n",
    "    print(f'Training Autoencoder with Latent Dimension: {latent_dim}')\n",
    "    \n",
    "    # Build model\n",
    "    autoencoder, encoder, decoder = build_compression_ae(latent_dim=latent_dim)\n",
    "    \n",
    "    # Compile\n",
    "    autoencoder.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=str(models_dir / f'compression_ae_latent{latent_dim}.keras'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=0\n",
    "        ),\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        ),\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=str(logs_dir / f'compression_latent{latent_dim}'),\n",
    "            histogram_freq=1\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    history = autoencoder.fit(\n",
    "        x_train, x_train,  # Input and target are the same\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test),\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    trained_models[latent_dim] = autoencoder\n",
    "    histories[latent_dim] = history\n",
    "    \n",
    "    print(f'\\nModel saved to: models/compression_ae_latent{latent_dim}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dce55be",
   "metadata": {},
   "source": [
    "### 3.2. Learning curves\n",
    "\n",
    "Let's examine how the models learned over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b203e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for all models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "for latent_dim, history in histories.items():\n",
    "\n",
    "    # Loss\n",
    "    axes[0].plot(history.history['loss'], label=f'Latent {latent_dim} (train)', alpha=0.7)\n",
    "    axes[0].plot(history.history['val_loss'], label=f'Latent {latent_dim} (val)', linestyle='--', alpha=0.7)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "for latent_dim, history in histories.items():\n",
    "    axes[1].plot(history.history['mae'], label=f'Latent {latent_dim} (train)', alpha=0.7)\n",
    "    axes[1].plot(history.history['val_mae'], label=f'Latent {latent_dim} (val)', linestyle='--', alpha=0.7)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Mean absolute error')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001868f",
   "metadata": {},
   "source": [
    "## 4. Model evaluation\n",
    "\n",
    "### 4.1. Reconstruction error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "evaluation_results = {}\n",
    "\n",
    "# Use a subset of test data for evaluation\n",
    "eval_samples = x_test[:1000]\n",
    "\n",
    "for latent_dim, model in trained_models.items():\n",
    "    print(f\"\\nEvaluating Latent Dim {latent_dim}...\")\n",
    "    \n",
    "    # Generate reconstructions\n",
    "    reconstructions = model.predict(eval_samples, verbose=0)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics_summary(eval_samples, reconstructions, latent_dim)\n",
    "    evaluation_results[latent_dim] = metrics\n",
    "    \n",
    "    print(f\"  MSE: {metrics['mse']:.6f}\")\n",
    "    print(f\"  PSNR: {metrics['psnr']:.2f} dB\")\n",
    "    print(f\"  SSIM: {metrics['ssim']:.4f}\")\n",
    "    print(f\"  Compression Ratio: {metrics['compression_ratio']:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reconstruction error distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "fig.suptitle('Reconstruction error distributions')\n",
    "\n",
    "# First pass: collect all errors to compute common bins\n",
    "all_errors = []\n",
    "\n",
    "for latent_dim in latent_dims:\n",
    "    model = trained_models[latent_dim]\n",
    "    reconstructions = model.predict(eval_samples, verbose=0)\n",
    "    errors = np.mean((eval_samples - reconstructions) ** 2, axis=(1, 2, 3))\n",
    "    all_errors.append(errors)\n",
    "\n",
    "# Compute common bins from combined data\n",
    "all_errors_combined = np.concatenate(all_errors)\n",
    "bins = np.histogram_bin_edges(all_errors_combined, bins=50)\n",
    "\n",
    "# Second pass: plot histograms with common bins\n",
    "for idx, latent_dim in enumerate(latent_dims):\n",
    "    errors = all_errors[idx]\n",
    "    \n",
    "    # Plot histogram with common bins\n",
    "    axes[idx].set_title(\n",
    "        f'Latent dim {latent_dim} ({3072/latent_dim:.1f}x compression)'\n",
    "    )\n",
    "    axes[idx].hist(errors, bins=bins, color='grey', edgecolor='black')\n",
    "    axes[idx].set_xlabel('Reconstruction MSE')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981c35f",
   "metadata": {},
   "source": [
    "## Visual Comparison of Reconstructions\n",
    "\n",
    "Let's visually compare the reconstruction quality across different latent dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random test images\n",
    "n_samples = 5\n",
    "sample_indices = np.random.choice(len(x_test), n_samples, replace=False)\n",
    "test_samples = x_test[sample_indices]\n",
    "\n",
    "# Create comparison figure\n",
    "fig, axes = plt.subplots(\n",
    "    len(latent_dims) + 1, n_samples, \n",
    "    figsize=(n_samples * 3, (len(latent_dims) + 1) * 3)\n",
    ")\n",
    "\n",
    "# Display originals\n",
    "for i in range(n_samples):\n",
    "    axes[0, i].imshow(test_samples[i])\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Original', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Display reconstructions for each latent dimension\n",
    "for row_idx, latent_dim in enumerate(latent_dims, start=1):\n",
    "    model = trained_models[latent_dim]\n",
    "    reconstructions = model.predict(test_samples, verbose=0)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        axes[row_idx, i].imshow(reconstructions[i])\n",
    "        axes[row_idx, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[row_idx, i].set_title(\n",
    "                f'Latent {latent_dim}\\n(Ratio: {3072/latent_dim:.1f}x)', \n",
    "                fontweight='bold', fontsize=14\n",
    "            )\n",
    "\n",
    "plt.suptitle('Reconstruction Quality Comparison', fontsize=18, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
